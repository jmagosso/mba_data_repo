{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5acdea19-bac0-4357-8a9e-5f1596b3ed78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CIDADES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3373e3a-c3a3-4bdd-8f3d-4f57670a42cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker\n",
    "\n",
    "from faker import Faker\n",
    "from pyspark.sql import Row\n",
    "\n",
    "fake = Faker('pt_BR')\n",
    "data = [\n",
    "    Row(\n",
    "        cidade_id=fake.random_int(min=1, max=9999),\n",
    "        nome=fake.city(),\n",
    "        uf=fake.estado_sigla(),\n",
    "        status_registro=1,\n",
    "        data_criacao=fake.date_this_decade().isoformat(),\n",
    "        data_atualizacao=fake.date_this_decade().isoformat()\n",
    "    )\n",
    "    for _ in range(10)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac53aa29-c2f8-487d-a239-64ee5b5a0545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "\n",
    "enderecos_data = []\n",
    "for row in df.collect():\n",
    "    for _ in range(3):\n",
    "        data_criacao = fake.date_time_this_decade()\n",
    "        data_atualizacao = data_criacao if random.random() < 0.7 else None\n",
    "        enderecos_data.append(\n",
    "            Row(\n",
    "                CEP=fake.postcode(),\n",
    "                logradouro=fake.street_suffix() if random.random() < 0.8 else None,\n",
    "                endereco=fake.street_name() if random.random() < 0.9 else None,\n",
    "                cidade_id=row.cidade_id,\n",
    "                status_registro=bool(random.getrandbits(1)),\n",
    "                data_criacao=data_criacao,\n",
    "                data_atualizacao=data_atualizacao\n",
    "            )\n",
    "        )\n",
    "\n",
    "df_enderecos = spark.createDataFrame(enderecos_data)\n",
    "display(df_enderecos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962493a6-526c-4e2b-8c3c-4762b23d39be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "categorias = [\n",
    "    \"RESTAURANTE\", \"SUPERMERCADO\", \"FARMACIA\", \"PADARIA\", \"LOJA DE ROUPAS\",\n",
    "    \"POSTO DE GASOLINA\", \"ACADEMIA\", \"PIZZARIA\", \"BAR\", \"SALAO DE BELEZA\"\n",
    "]\n",
    "status_categorias = [\"A\", \"B\", \"D\", \"P\", \"H\"]\n",
    "\n",
    "categoria_data = []\n",
    "for i, desc in enumerate(categorias, start=1):\n",
    "    data_criacao = fake.date_time_this_decade()\n",
    "    data_atualizacao = (\n",
    "        data_criacao + timedelta(days=random.randint(1, 100))\n",
    "        if random.random() < 0.5 else None\n",
    "    )\n",
    "    categoria_data.append(\n",
    "        Row(\n",
    "            categoria_id=i,\n",
    "            desc_categoria=desc,\n",
    "            status_categoria=random.choice(status_categorias),\n",
    "            status_registro=bool(random.getrandbits(1)),\n",
    "            data_criacao=data_criacao,\n",
    "            data_atualizacao=data_atualizacao\n",
    "        )\n",
    "    )\n",
    "\n",
    "df_categoria_estabelecimentos = spark.createDataFrame(categoria_data)\n",
    "display(df_categoria_estabelecimentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad7b6968-ceef-48b1-8e46-971c4932a9c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import timedelta\n",
    "\n",
    "tipos_fiscais = [\n",
    "    (\"MEI\", \"Microempreendedor Individual\"),\n",
    "    (\"LTDA\", \"Sociedade Limitada\"),\n",
    "    (\"EIRELI\", \"Empresa Individual de Responsabilidade Limitada\"),\n",
    "    (\"EI\", \"Empresário Individual\"),\n",
    "    (\"SA\", \"Sociedade Anônima\"),\n",
    "    (\"SIMPLES\", \"Simples Nacional\"),\n",
    "    (\"SCP\", \"Sociedade em Conta de Participação\"),\n",
    "    (\"SNC\", \"Sociedade em Nome Coletivo\"),\n",
    "    (\"SLU\", \"Sociedade Limitada Unipessoal\"),\n",
    "    (\"ME\", \"Microempresa\")\n",
    "]\n",
    "\n",
    "tipo_fiscal_data = []\n",
    "for i, (code, desc) in enumerate(tipos_fiscais, start=1):\n",
    "    data_criacao = fake.date_time_this_decade()\n",
    "    data_atualizacao = (\n",
    "        data_criacao + timedelta(days=random.randint(1, 100))\n",
    "        if random.random() < 0.5 else None\n",
    "    )\n",
    "    tipo_fiscal_data.append(\n",
    "        Row(\n",
    "            fiscal_id=i,\n",
    "            code_fiscal=code,\n",
    "            desc_fiscal=desc,\n",
    "            status_registro=bool(random.getrandbits(1)),\n",
    "            data_criacao=data_criacao,\n",
    "            data_atualizacao=data_atualizacao\n",
    "        )\n",
    "    )\n",
    "\n",
    "df_tipo_fiscal = spark.createDataFrame(tipo_fiscal_data)\n",
    "display(df_tipo_fiscal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fc42fc-584b-4783-aeed-db383928b2c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "\n",
    "estabelecimentos_data = []\n",
    "enderecos = df_enderecos.collect()\n",
    "categorias = df_categoria_estabelecimentos.collect()\n",
    "fiscais = df_tipo_fiscal.collect()\n",
    "status_estab = [\"A\", \"B\", \"D\", \"P\", \"H\"]\n",
    "\n",
    "for i in range(1, 21):\n",
    "    endereco = random.choice(enderecos)\n",
    "    categoria = random.choice(categorias)\n",
    "    fiscal = random.choice(fiscais)\n",
    "    data_criacao = fake.date_time_this_decade()\n",
    "    data_atualizacao = (\n",
    "        data_criacao if random.random() < 0.7 else None\n",
    "    )\n",
    "    estabelecimentos_data.append(\n",
    "        Row(\n",
    "            estab_id=i,\n",
    "            num_cnpj=fake.cnpj(),\n",
    "            razao_social=fake.company(),\n",
    "            nome_fantasia=fake.company_suffix() if random.random() < 0.8 else None,\n",
    "            categoria_id=categoria.categoria_id,\n",
    "            fiscal_id=fiscal.fiscal_id,\n",
    "            CEP=endereco.CEP,\n",
    "            numero_endereco=str(fake.building_number()) if random.random() < 0.9 else None,\n",
    "            complemento=\"\",\n",
    "            status_estabelecimento=random.choice(status_estab),\n",
    "            status_registro=bool(random.getrandbits(1)),\n",
    "            data_criacao=data_criacao,\n",
    "            data_atualizacao=data_atualizacao\n",
    "        )\n",
    "    )\n",
    "\n",
    "display(estabelecimentos_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee9c34b-9839-4d95-ab8c-da1a7a642d98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "fabricantes_nomes = [\n",
    "    \"Stone Pagamentos\", \"Cielo\", \"Rede\", \"Getnet\", \"PagSeguro\"\n",
    "]\n",
    "status_pos_list = [\"A\", \"B\", \"D\", \"P\", \"H\"]\n",
    "\n",
    "fabricantes_data = []\n",
    "for i, nome in enumerate(fabricantes_nomes, start=1):\n",
    "    data_criacao = fake.date_time_this_decade()\n",
    "    data_atualizacao = (\n",
    "        data_criacao + timedelta(days=random.randint(1, 100))\n",
    "        if random.random() < 0.5 else None\n",
    "    )\n",
    "    fabricantes_data.append(\n",
    "        Row(\n",
    "            fabricante_id=i,\n",
    "            nome_fabricante=nome,\n",
    "            status_pos=random.choice(status_pos_list),\n",
    "            status_registro=bool(random.getrandbits(1)),\n",
    "            data_criacao=data_criacao,\n",
    "            data_atualizacao=data_atualizacao\n",
    "        )\n",
    "    )\n",
    "\n",
    "df_fabricantes_pos = spark.createDataFrame(fabricantes_data)\n",
    "display(df_fabricantes_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc370ad-0965-42ba-abfb-7237fb1ddf4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "modelos_pos = [\n",
    "    \"Ingenico Move/5000\", \"Verifone VX 520\", \"PAX A920\", \"Gertec PPC930\", \"Ingenico iWL250\",\n",
    "    \"Verifone VX 680\", \"PAX D210\", \"Gertec MP35P\", \"Ingenico Desk/5000\", \"Verifone V200c\"\n",
    "]\n",
    "tecnologias_pos = [\n",
    "    \"NFC\", \"Chip\", \"Magstripe\", \"QR Code\", \"Bluetooth\", \"Wi-Fi\", \"3G\", \"4G\", None\n",
    "]\n",
    "fabricantes = df_fabricantes_pos.collect()\n",
    "status_pos_list = [\"A\", \"B\", \"D\", \"P\", \"H\"]\n",
    "\n",
    "ponto_venda_data = []\n",
    "for i in range(1, 21):\n",
    "    modelo = random.choice(modelos_pos)\n",
    "    tecnologia = random.choice(tecnologias_pos)\n",
    "    fabricante = random.choice(fabricantes)\n",
    "    data_criacao = fake.date_time_this_decade()\n",
    "    data_atualizacao = (\n",
    "        data_criacao + timedelta(days=random.randint(1, 100))\n",
    "        if random.random() < 0.5 else None\n",
    "    )\n",
    "    ponto_venda_data.append(\n",
    "        Row(\n",
    "            pos_id=i,\n",
    "            modelo_pos=modelo,\n",
    "            tecnologia_pos=tecnologia,\n",
    "            fabricante_id=fabricante.fabricante_id,\n",
    "            status_pos=random.choice(status_pos_list),\n",
    "            status_registro=bool(random.getrandbits(1)),\n",
    "            data_criacao=data_criacao,\n",
    "            data_atualizacao=data_atualizacao\n",
    "        )\n",
    "    )\n",
    "\n",
    "df_ponto_venda = spark.createDataFrame(ponto_venda_data)\n",
    "display(df_ponto_venda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296ce4c6-5cda-492b-81d6-c2de6fab7ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "estabelecimentos = estabelecimentos_data  # já é uma lista de Row\n",
    "pos_list = df_ponto_venda.collect()\n",
    "status_estab_pos_list = [\"A\", \"B\", \"D\", \"P\", \"H\", \"R\"]\n",
    "\n",
    "estab_pos_data = []\n",
    "estab_pos_id = 1\n",
    "\n",
    "for estab in estabelecimentos:\n",
    "    num_pos = random.randint(1, 3)\n",
    "    pos_ids = random.sample([p.pos_id for p in pos_list], k=num_pos)\n",
    "    for pos_id in pos_ids:\n",
    "        data_criacao = fake.date_time_this_decade()\n",
    "        data_ativacao = data_criacao + timedelta(days=random.randint(0, 30)) if random.random() < 0.8 else None\n",
    "        data_desativacao = (\n",
    "            data_ativacao + timedelta(days=random.randint(1, 365))\n",
    "            if data_ativacao and random.random() < 0.2 else None\n",
    "        )\n",
    "        data_atualizacao = (\n",
    "            data_criacao + timedelta(days=random.randint(1, 100))\n",
    "            if random.random() < 0.5 else None\n",
    "        )\n",
    "        permite_debito = bool(random.getrandbits(1))\n",
    "        permite_credito = bool(random.getrandbits(1))\n",
    "        if permite_credito:\n",
    "            num_parcela = random.randint(1, 3)\n",
    "            num_parcela_juros = random.randint(4, 10)\n",
    "        else:\n",
    "            num_parcela = None\n",
    "            num_parcela_juros = None\n",
    "        # Regra de taxa de juros conforme pedido\n",
    "        if permite_credito and num_parcela > 3:\n",
    "            taxa_juros = 10.0\n",
    "        else:\n",
    "            taxa_juros = 0.0\n",
    "        taxa_mdr = round(random.uniform(1.0, 4.0), 2)\n",
    "        taxa_rav = round(random.uniform(0.5, 3.0), 2)\n",
    "        equipamento_alugado = random.choice([1, 2])\n",
    "        status_estab_pos = random.choice(status_estab_pos_list)\n",
    "        status_registro = bool(random.getrandbits(1))\n",
    "        estab_pos_data.append(\n",
    "            Row(\n",
    "                estab_pos_id=estab_pos_id,\n",
    "                estab_id=estab.estab_id,\n",
    "                pos_id=pos_id,\n",
    "                data_ativacao=data_ativacao,\n",
    "                data_desativacao=data_desativacao,\n",
    "                permite_debito=permite_debito,\n",
    "                permite_credito=permite_credito,\n",
    "                num_parcela=num_parcela,\n",
    "                num_parcela_juros=num_parcela_juros,\n",
    "                taxa_juros=taxa_juros,\n",
    "                taxa_mdr=taxa_mdr,\n",
    "                taxa_rav=taxa_rav,\n",
    "                equipamento_alugado=equipamento_alugado,\n",
    "                status_estab_pos=status_estab_pos,\n",
    "                status_registro=status_registro,\n",
    "                data_criacao=data_criacao,\n",
    "                data_atualizacao=data_atualizacao\n",
    "            )\n",
    "        )\n",
    "        estab_pos_id += 1\n",
    "        if estab_pos_id > 100:\n",
    "            break\n",
    "    if estab_pos_id > 100:\n",
    "        break\n",
    "\n",
    "df_estabelecimento_pos = spark.createDataFrame(estab_pos_data)\n",
    "display(df_estabelecimento_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392c105f-2f2b-4864-b197-709b7e87a4cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "bandeiras_cartao = [\"Visa\", \"Mastercard\", \"Elo\", \"Hipercard\", \"Amex\", \"Cabal\", \"Sorocred\", None]\n",
    "status_transacao_list = [\"A\", \"P\", \"C\", \"N\"]\n",
    "\n",
    "# Map estab_pos_id to (permite_credito, num_parcela, num_parcela_juros)\n",
    "estab_pos_info = {\n",
    "    row.estab_pos_id: (row.permite_credito, row.num_parcela, row.num_parcela_juros)\n",
    "    for row in df_estabelecimento_pos.collect()\n",
    "}\n",
    "estab_pos_ids = list(estab_pos_info.keys())\n",
    "transacoes_data = []\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    estab_pos_id = random.choice(estab_pos_ids)\n",
    "    permite_credito, num_parcela, num_parcela_juros = estab_pos_info[estab_pos_id]\n",
    "    data_transacao = fake.date_time_this_decade()\n",
    "    valor_transacao = round(random.uniform(10.0, 2000.0), 2)\n",
    "    tipo_transacao = random.choice([0, 1])  # 0 = Débito, 1 = Crédito\n",
    "\n",
    "    if tipo_transacao == 0 or not permite_credito:\n",
    "        num_parcelas = 0\n",
    "    else:\n",
    "        # Respeita o limite de parcelas permitido pelo estabelecimento\n",
    "        max_parcelas = num_parcela if num_parcela is not None else 1\n",
    "        max_parcelas = max(1, max_parcelas)\n",
    "        num_parcelas = random.randint(1, max_parcelas)\n",
    "\n",
    "    bandeira_cartao = random.choice(bandeiras_cartao)\n",
    "    bin_cartao = fake.credit_card_number()[:6] if bandeira_cartao else None\n",
    "    status_transacao = random.choice(status_transacao_list)\n",
    "    status_registro = bool(random.getrandbits(1))\n",
    "    data_criacao = data_transacao - timedelta(days=random.randint(0, 10))\n",
    "    data_atualizacao = (\n",
    "        data_criacao + timedelta(days=random.randint(1, 10))\n",
    "        if random.random() < 0.3 else None\n",
    "    )\n",
    "    transacoes_data.append(\n",
    "        Row(\n",
    "            transacao_id=i,\n",
    "            estab_pos_id=estab_pos_id,\n",
    "            data_transacao=data_transacao,\n",
    "            valor_transacao=valor_transacao,\n",
    "            tipo_transacao=tipo_transacao,\n",
    "            num_parcelas=num_parcelas,\n",
    "            bandeira_cartao=bandeira_cartao,\n",
    "            bin_cartao=bin_cartao,\n",
    "            status_transacao=status_transacao,\n",
    "            status_registro=status_registro,\n",
    "            data_criacao=data_criacao,\n",
    "            data_atualizacao=data_atualizacao\n",
    "        )\n",
    "    )\n",
    "\n",
    "df_transacoes = spark.createDataFrame(transacoes_data)\n",
    "display(df_transacoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d05b51-fbbe-40ec-b3a6-3899e807619e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "transacoes_parcelas_data = []\n",
    "status_parcela_list = [\"A\", \"P\", \"X\"]\n",
    "\n",
    "# Seleciona as transações de crédito, garantindo que 394 e 405 estejam presentes\n",
    "transacoes_parcelas = (\n",
    "    df_transacoes\n",
    "    .filter(\"num_parcelas > 0\")\n",
    "    .orderBy(\"transacao_id\")\n",
    "    .limit(100)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Garante que as transações 394 e 405 estejam na lista\n",
    "ids_necessarios = {394, 405}\n",
    "transacoes_ids = {t.transacao_id for t in transacoes_parcelas}\n",
    "faltantes = ids_necessarios - transacoes_ids\n",
    "if faltantes:\n",
    "    transacoes_faltantes = (\n",
    "        df_transacoes\n",
    "        .filter((col(\"transacao_id\").isin([int(i) for i in faltantes])) & (col(\"num_parcelas\") > 0))\n",
    "        .collect()\n",
    "    )\n",
    "    transacoes_parcelas += [t for t in transacoes_faltantes if t.transacao_id not in transacoes_ids]\n",
    "\n",
    "transacao_parc_id = 1\n",
    "for trans in transacoes_parcelas:\n",
    "    valor_total = trans.valor_transacao\n",
    "    # Força 8 parcelas para transacao_id 394\n",
    "    if trans.transacao_id == 394:\n",
    "        num_parcelas = 8\n",
    "    else:\n",
    "        num_parcelas = trans.num_parcelas\n",
    "    data_base = trans.data_transacao\n",
    "    for codigo_parcela in range(1, num_parcelas + 1):\n",
    "        valor_parcela = round(valor_total / num_parcelas, 2)\n",
    "        valor_juros = round(valor_parcela * random.uniform(0.01, 0.05), 2)\n",
    "        valor_mdr = round(valor_parcela * random.uniform(0.01, 0.04), 2)\n",
    "        data_vencimento = data_base + timedelta(days=30 * codigo_parcela)\n",
    "        status_parcela = random.choice(status_parcela_list)\n",
    "        status_registro = bool(random.getrandbits(1))\n",
    "        data_criacao = data_base - timedelta(days=random.randint(0, 5))\n",
    "        data_atualizacao = (\n",
    "            data_criacao + timedelta(days=random.randint(1, 5))\n",
    "            if random.random() < 0.3 else None\n",
    "        )\n",
    "        transacoes_parcelas_data.append(\n",
    "            Row(\n",
    "                transacao_parc_id=transacao_parc_id,\n",
    "                transacao_id=trans.transacao_id,\n",
    "                codigo_parcela=codigo_parcela,\n",
    "                valor_parcela=valor_parcela,\n",
    "                valor_juros=valor_juros,\n",
    "                valor_mdr=valor_mdr,\n",
    "                data_vencimento=data_vencimento,\n",
    "                status_parcela=status_parcela,\n",
    "                status_registro=status_registro,\n",
    "                data_criacao=data_criacao,\n",
    "                data_atualizacao=data_atualizacao\n",
    "            )\n",
    "        )\n",
    "        transacao_parc_id += 1\n",
    "        if transacao_parc_id > 100:\n",
    "            break\n",
    "    if transacao_parc_id > 100:\n",
    "        break\n",
    "\n",
    "df_transacoes_parcelas = spark.createDataFrame(transacoes_parcelas_data)\n",
    "display(df_transacoes_parcelas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c85577a-6615-4df6-8ade-9b91cc7ebbbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "status_antecipacao_list = [\"A\", \"B\", \"R\", \"P\"]\n",
    "\n",
    "# Seleciona aleatoriamente algumas parcelas para antecipação\n",
    "parcelas = df_transacoes_parcelas.collect()\n",
    "num_antecipacoes = min(40, len(parcelas))\n",
    "parcelas_antecipadas = random.sample(parcelas, num_antecipacoes)\n",
    "\n",
    "antecipacoes_data = []\n",
    "antecipacao_id = 1\n",
    "\n",
    "for parcela in parcelas_antecipadas:\n",
    "    dias_antecipados = random.randint(1, 25)\n",
    "    data_vencimento_original = parcela.data_vencimento\n",
    "    data_vencimento_antecipado = data_vencimento_original - timedelta(days=dias_antecipados)\n",
    "    valor_parcela = parcela.valor_parcela\n",
    "    valor_juros = parcela.valor_juros\n",
    "    valor_mdr = parcela.valor_mdr\n",
    "    valor_rav = round(valor_parcela * random.uniform(0.01, 0.03), 2)\n",
    "    valor_liquido = round(valor_parcela - valor_juros - valor_mdr - valor_rav, 2)\n",
    "    status_antecipacao = random.choice(status_antecipacao_list)\n",
    "    status_registro = bool(random.getrandbits(1))\n",
    "    data_criacao = data_vencimento_antecipado - timedelta(days=random.randint(0, 5))\n",
    "    data_atualizacao = (\n",
    "        data_criacao + timedelta(days=random.randint(1, 5))\n",
    "        if random.random() < 0.3 else None\n",
    "    )\n",
    "    antecipacoes_data.append(\n",
    "        Row(\n",
    "            antecipacao_id=antecipacao_id,\n",
    "            transacao_parc_id=parcela.transacao_parc_id,\n",
    "            valor_parcela=valor_parcela,\n",
    "            valor_juros=valor_juros,\n",
    "            valor_mdr=valor_mdr,\n",
    "            valor_rav=valor_rav,\n",
    "            valor_liquido=valor_liquido,\n",
    "            data_vencimento_original=data_vencimento_original,\n",
    "            data_vencimento_antecipado=data_vencimento_antecipado,\n",
    "            nr_dias_antecipados=dias_antecipados,\n",
    "            status_antecipacao=status_antecipacao,\n",
    "            status_registro=status_registro,\n",
    "            data_criacao=data_criacao,\n",
    "            data_atualizacao=data_atualizacao\n",
    "        )\n",
    "    )\n",
    "    antecipacao_id += 1\n",
    "\n",
    "df_antecipacoes = spark.createDataFrame(antecipacoes_data)\n",
    "display(df_antecipacoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9ab9de-9203-480a-b189-77f8d498b9e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "# Coleta transações de débito (num_parcelas == 0) e parcelas de crédito (num_parcelas > 0)\n",
    "transacoes_debito = (\n",
    "    df_transacoes\n",
    "    .filter(\"num_parcelas == 0\")\n",
    "    .orderBy(\"data_transacao\")\n",
    "    .limit(100)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "parcelas_credito = df_transacoes_parcelas.orderBy(\"data_vencimento\").limit(100).collect()\n",
    "\n",
    "# Junta e embaralha para simular agrupamentos de recebimentos\n",
    "transacoes_recebiveis = [\n",
    "    {\n",
    "        \"tipo\": \"debito\",\n",
    "        \"transacao_id\": t.transacao_id,\n",
    "        \"estab_pos_id\": t.estab_pos_id,\n",
    "        \"data_recebimento\": t.data_transacao.date(),\n",
    "        \"valor_bruto\": t.valor_transacao,\n",
    "        \"valor_juros\": 0.0,\n",
    "        \"valor_mdr\": round(t.valor_transacao * random.uniform(0.01, 0.04), 2),\n",
    "        \"valor_rav\": 0.0,\n",
    "    }\n",
    "    for t in transacoes_debito\n",
    "] + [\n",
    "    {\n",
    "        \"tipo\": \"credito\",\n",
    "        \"transacao_id\": p.transacao_id,\n",
    "        \"transacao_parc_id\": p.transacao_parc_id,\n",
    "        \"estab_pos_id\": (\n",
    "            df_transacoes\n",
    "            .filter(f\"transacao_id = {p.transacao_id}\")\n",
    "            .select(\"estab_pos_id\")\n",
    "            .first()\n",
    "            .estab_pos_id\n",
    "        ),\n",
    "        \"data_recebimento\": p.data_vencimento.date(),\n",
    "        \"valor_bruto\": p.valor_parcela + p.valor_juros,\n",
    "        \"valor_juros\": p.valor_juros,\n",
    "        \"valor_mdr\": p.valor_mdr,\n",
    "        \"valor_rav\": round(p.valor_parcela * random.uniform(0.01, 0.03), 2),\n",
    "    }\n",
    "    for p in parcelas_credito\n",
    "]\n",
    "\n",
    "random.shuffle(transacoes_recebiveis)\n",
    "\n",
    "# Agrupa por estabelecimento e data_recebimento para simular títulos\n",
    "estab_pos_to_estab = {\n",
    "    row.estab_pos_id: row.estab_id\n",
    "    for row in df_estabelecimento_pos.collect()\n",
    "}\n",
    "\n",
    "recebimentos_data = []\n",
    "recebimento_id = 1\n",
    "num_recebimento = 1000\n",
    "\n",
    "agrupados = {}\n",
    "for t in transacoes_recebiveis:\n",
    "    estab_id = estab_pos_to_estab[t[\"estab_pos_id\"]]\n",
    "    key = (estab_id, t[\"data_recebimento\"])\n",
    "    if key not in agrupados:\n",
    "        agrupados[key] = []\n",
    "    agrupados[key].append(t)\n",
    "\n",
    "for (estab_id, data_recebimento), trans_list in list(agrupados.items())[:50]:\n",
    "    quantidade_transacoes = len(trans_list)\n",
    "    valor_titulo_bruto = sum(t[\"valor_bruto\"] for t in trans_list)\n",
    "    valor_mdr = sum(t[\"valor_mdr\"] for t in trans_list)\n",
    "    valor_rav = sum(t[\"valor_rav\"] for t in trans_list)\n",
    "    valor_pos = round(random.uniform(0, 50), 2) if random.random() < 0.2 else 0.0\n",
    "    valor_liquido = round(valor_titulo_bruto - valor_mdr - valor_rav - valor_pos, 2)\n",
    "    status_recebimento = random.choice([\"A\", \"C\"])\n",
    "    status_registro = bool(random.getrandbits(1))\n",
    "    data_criacao = data_recebimento - timedelta(days=random.randint(0, 5))\n",
    "    data_atualizacao = (\n",
    "        data_criacao + timedelta(days=random.randint(1, 5))\n",
    "        if random.random() < 0.3 else None\n",
    "    )\n",
    "    recebimentos_data.append(\n",
    "        Row(\n",
    "            recebimento_id=recebimento_id,\n",
    "            num_recebimento=num_recebimento,\n",
    "            data_recebimento=data_recebimento,\n",
    "            estab_id=str(estab_id),\n",
    "            quantidade_transacoes=quantidade_transacoes,\n",
    "            valor_titulo_bruto=valor_titulo_bruto,\n",
    "            valor_mdr=valor_mdr,\n",
    "            valor_rav=valor_rav,\n",
    "            valor_pos=valor_pos,\n",
    "            valor_liquido=valor_liquido,\n",
    "            status_recebimento=status_recebimento,\n",
    "            status_registro=status_registro,\n",
    "            data_criacao=data_criacao,\n",
    "            data_atualizacao=data_atualizacao\n",
    "        )\n",
    "    )\n",
    "    recebimento_id += 1\n",
    "    num_recebimento += 1\n",
    "\n",
    "df_recebimentos = spark.createDataFrame(recebimentos_data)\n",
    "display(df_recebimentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2bdc4b2-2b4d-4bec-86df-5c64334a3bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, monotonically_increasing_id\n",
    "\n",
    "# Cria um dicionário para mapear (estab_id, data_recebimento) para recebimento_id\n",
    "recebimento_map = {\n",
    "    (int(row.estab_id), row.data_recebimento): row.recebimento_id\n",
    "    for row in df_recebimentos.collect()\n",
    "}\n",
    "\n",
    "# Cria um dicionário para mapear transacao_id para transacao_parc_id (para crédito)\n",
    "transacao_parc_map = {\n",
    "    (row.transacao_id, row.data_vencimento.date()): row.transacao_parc_id\n",
    "    for row in df_transacoes_parcelas.collect()\n",
    "}\n",
    "\n",
    "# Lista para armazenar os registros Recebimento_Transacoes\n",
    "recebimento_trn_data = []\n",
    "recebimento_trn_id = 1\n",
    "\n",
    "for row in df_recebimentos.collect():\n",
    "    estab_id = int(row.estab_id)\n",
    "    data_recebimento = row.data_recebimento\n",
    "    recebimento_id = row.recebimento_id\n",
    "\n",
    "    # Busca transações de débito\n",
    "    debito_transacoes = [\n",
    "        t for t in df_transacoes\n",
    "        .filter((col(\"estab_pos_id\").isin(\n",
    "            [k for k, v in estab_pos_to_estab.items() if v == estab_id]\n",
    "        )) & (col(\"num_parcelas\") == 0) & (col(\"data_transacao\").cast(\"date\") == data_recebimento))\n",
    "        .collect()\n",
    "    ]\n",
    "    # Não há transacao_parc_id para débito, então pula\n",
    "\n",
    "    # Busca parcelas de crédito\n",
    "    credito_parcelas = [\n",
    "        p for p in df_transacoes_parcelas\n",
    "        .filter((col(\"data_vencimento\").cast(\"date\") == data_recebimento))\n",
    "        .join(df_transacoes, \"transacao_id\")\n",
    "        .filter(col(\"estab_pos_id\").isin(\n",
    "            [k for k, v in estab_pos_to_estab.items() if v == estab_id]\n",
    "        ))\n",
    "        .select(\"transacao_parc_id\", \"status_registro\", \"data_criacao\", \"data_atualizacao\")\n",
    "        .collect()\n",
    "    ]\n",
    "\n",
    "    for p in credito_parcelas:\n",
    "        recebimento_trn_data.append(\n",
    "            Row(\n",
    "                recebimento_trn_id=recebimento_trn_id,\n",
    "                recebimento_id=recebimento_id,\n",
    "                transacao_parc_id=p.transacao_parc_id,\n",
    "                status_registro=p.status_registro,\n",
    "                data_criacao=p.data_criacao,\n",
    "                data_atualizacao=p.data_atualizacao\n",
    "            )\n",
    "        )\n",
    "        recebimento_trn_id += 1\n",
    "\n",
    "df_recebimento_transacoes = spark.createDataFrame(recebimento_trn_data)\n",
    "display(df_recebimento_transacoes)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fake_data_mdb_AdvancedModeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
