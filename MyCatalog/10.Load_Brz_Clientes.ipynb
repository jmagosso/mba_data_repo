{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0dfd88-7bea-45b2-9a65-8114438a6488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker\n",
    "\n",
    "from faker import Faker\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.functions import current_timestamp, from_utc_timestamp\n",
    "import pandas as pd\n",
    "\n",
    "spark.sql(\"TRUNCATE TABLE data_catalog_01_d.bronze.clientes\")\n",
    "\n",
    "fake = Faker('pt_BR')\n",
    "data = []\n",
    "for i in range(10000):\n",
    "    data.append({\n",
    "        \"id\": i + 1,\n",
    "        \"cnpj\": fake.cnpj(),\n",
    "        \"razao_social\": fake.company(),\n",
    "        \"fantasia\": fake.company_suffix(),\n",
    "        \"cidade\": fake.city(),\n",
    "        \"endereco\": fake.street_name(),\n",
    "        \"numero\": fake.building_number(),\n",
    "        \"estado\": fake.estado_sigla(),\n",
    "        \"vendedor\": fake.name(),\n",
    "    })\n",
    "\n",
    "df_pd = pd.DataFrame(data)\n",
    "df_spark = spark.createDataFrame(df_pd)\n",
    "df_spark = df_spark.withColumn(\"data_carga\",from_utc_timestamp(current_timestamp(), \"America/Sao_Paulo\"))\n",
    "df_spark.write.format(\"delta\").mode(\"append\").saveAsTable(\"data_catalog_01_d.bronze.clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4be782f-eb1f-463f-927f-eb205ff1dca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adiciona 15 novos registros à tabela\n",
    "novos_dados = []\n",
    "for i in range(2):\n",
    "    novos_dados.append({\n",
    "        \"id\": None,  # Será ajustado abaixo\n",
    "        \"cnpj\": fake.cnpj(),\n",
    "        \"razao_social\": fake.company(),\n",
    "        \"fantasia\": fake.company_suffix(),\n",
    "        \"cidade\": fake.city(),\n",
    "        \"endereco\": fake.street_name(),\n",
    "        \"numero\": fake.building_number(),\n",
    "        \"estado\": fake.estado_sigla(),\n",
    "        \"vendedor\": fake.name(),\n",
    "    })\n",
    "\n",
    "# Descobre o maior ID atual para continuar a sequência\n",
    "max_id = spark.sql(\"SELECT MAX(id) as max_id FROM data_catalog_01_d.bronze.clientes\").collect()[0]['max_id'] or 0\n",
    "for idx, row in enumerate(novos_dados):\n",
    "    row['id'] = max_id + idx + 1\n",
    "\n",
    "df_novos = pd.DataFrame(novos_dados)\n",
    "df_novos_spark = spark.createDataFrame(df_novos)\n",
    "df_novos_spark = df_novos_spark.withColumn(\"data_carga\",from_utc_timestamp(current_timestamp(), \"America/Sao_Paulo\"))\n",
    "df_novos_spark.write.format(\"delta\").mode(\"append\").saveAsTable(\"data_catalog_01_d.bronze.clientes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5c2481-4136-4b64-a0cf-3d61fa246fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Seleciona aleatoriamente alguns IDs existentes para recarregar com novo nome de vendedor\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "ids_aleatorios = (\n",
    "    spark.table(\"data_catalog_01_d.bronze.clientes\")\n",
    "    .orderBy(rand())\n",
    "    .limit(5)\n",
    "    .select(\"id\")\n",
    "    .toPandas()[\"id\"].tolist()\n",
    ")\n",
    "\n",
    "# Para cada ID selecionado, recarrega o registro com novo nome de vendedor e atualiza data_carga\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "for id_alterar in ids_aleatorios:\n",
    "    registro = (\n",
    "        spark.table(\"data_catalog_01_d.bronze.clientes\")\n",
    "        .filter(f\"id = {id_alterar}\")\n",
    "        .orderBy(\"data_carga\", ascending=False)\n",
    "        .limit(1)\n",
    "    )\n",
    "    registro = registro.withColumn(\"vendedor\", lit(fake.name()))\n",
    "    registro = registro.withColumn(\"data_carga\",from_utc_timestamp(current_timestamp(), \"America/Sao_Paulo\"))\n",
    "    registro.write.format(\"delta\").mode(\"append\").saveAsTable(\"data_catalog_01_d.bronze.clientes\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8825079445933115,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "10.Load_Brz_Clientes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
